{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "  for W, b in params:\n",
    "    outputs = np.dot(inputs, W) + b\n",
    "    inputs = np.tanh(outputs)\n",
    "  return outputs\n",
    "\n",
    "def logprob_fun(params, inputs, targets):\n",
    "  preds = predict(params, inputs)\n",
    "  return np.sum((preds - targets)**2)\n",
    "\n",
    "grad_fun = jax.jit(jax.grad(logprob_fun))  # compiled gradient evaluation function\n",
    "perex_grads = jax.jit(jax.vmap(grad_fun, in_axes=(None, 0, 0)))  # fast per-example grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Computes our network's output\n",
    "def net(params, x):\n",
    "    w1, b1, w2, b2 = params\n",
    "    hidden = np.tanh(np.dot(w1, x) + b1)\n",
    "    return sigmoid(np.dot(w2, hidden) + b2)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def loss(params, x, y):\n",
    "    out = net(params, x)\n",
    "    cross_entropy = -y * np.log(out) - (1 - y)*np.log(1 - out)\n",
    "    return cross_entropy\n",
    "\n",
    "# Utility function for testing whether the net produces the correct\n",
    "# output for all possible inputs\n",
    "def test_all_inputs(inputs, params):\n",
    "    predictions = [int(net(params, inp) > 0.5) for inp in inputs]\n",
    "    for inp, out in zip(inputs, predictions):\n",
    "        print(inp, '->', out)\n",
    "    return (predictions == [onp.bitwise_xor(*inp) for inp in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_params():\n",
    "    return [\n",
    "        onp.random.randn(3, 2),  # w1\n",
    "        onp.random.randn(3),  # b1\n",
    "        onp.random.randn(3),  # w2\n",
    "        onp.random.randn(),  #b2\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 1\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 0\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n",
      "Iteration 200\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "loss_grad = jax.jit(jax.grad(loss))\n",
    "\n",
    "# Stochastic gradient descent learning rate\n",
    "learning_rate = 1.\n",
    "# All possible inputs\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Initialize parameters randomly\n",
    "params = initial_params()\n",
    "\n",
    "for n in itertools.count():\n",
    "    # Grab a single random input\n",
    "    x = inputs[onp.random.choice(inputs.shape[0])]\n",
    "    # Compute the target output\n",
    "    y = onp.bitwise_xor(*x)\n",
    "    # Get the gradient of the loss for this input/output pair\n",
    "    grads = loss_grad(params, x, y)\n",
    "    # Update parameters via gradient descent\n",
    "    params = [param - learning_rate * grad\n",
    "              for param, grad in zip(params, grads)]\n",
    "    # Every 100 iterations, check whether we've solved XOR\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 1\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "source": [
    "loss_grad = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0), out_axes=0))\n",
    "\n",
    "params = initial_params()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for n in itertools.count():\n",
    "    # Generate a batch of inputs\n",
    "    x = inputs[onp.random.choice(inputs.shape[0], size=batch_size)]\n",
    "    y = onp.bitwise_xor(x[:, 0], x[:, 1])\n",
    "    # The call to loss_grad remains the same!\n",
    "    grads = loss_grad(params, x, y)\n",
    "    # Note that we now need to average gradients over the batch\n",
    "    params = [param - learning_rate * np.mean(grad, axis=0)\n",
    "              for param, grad in zip(params, grads)]\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting objax\n",
      "  Downloading objax-1.3.1.tar.gz (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 957 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/silviu/anaconda3/lib/python3.6/site-packages (from objax) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from objax) (1.19.5)\n",
      "Requirement already satisfied: pillow in /home/silviu/anaconda3/lib/python3.6/site-packages (from objax) (8.1.0)\n",
      "Requirement already satisfied: jaxlib in /home/silviu/anaconda3/lib/python3.6/site-packages (from objax) (0.1.61+cuda101)\n",
      "Requirement already satisfied: jax in /home/silviu/anaconda3/lib/python3.6/site-packages (from objax) (0.2.9)\n",
      "Requirement already satisfied: tensorboard>=2.3.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from objax) (2.4.0)\n",
      "Collecting parameterized\n",
      "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (3.1.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (3.14.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (1.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (52.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (0.16.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (1.7.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (0.9.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/silviu/anaconda3/lib/python3.6/site-packages (from tensorboard>=2.3.0->objax) (1.25.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax) (2.0.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /home/silviu/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax) (3.4.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/silviu/anaconda3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax) (0.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->objax) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /home/silviu/anaconda3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/silviu/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax) (1.25.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/silviu/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/silviu/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/silviu/anaconda3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax) (2020.6.20)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/silviu/anaconda3/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->objax) (3.1.0)\n",
      "Requirement already satisfied: opt-einsum in /home/silviu/anaconda3/lib/python3.6/site-packages (from jax->objax) (3.1.0)\n",
      "Requirement already satisfied: flatbuffers in /home/silviu/anaconda3/lib/python3.6/site-packages (from jaxlib->objax) (1.12)\n",
      "Building wheels for collected packages: objax\n",
      "  Building wheel for objax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for objax: filename=objax-1.3.1-py3-none-any.whl size=72554 sha256=616d559eefc2a762748425af9dd99f9d8630930551652be7945472b8900c362c\n",
      "  Stored in directory: /home/silviu/.cache/pip/wheels/8f/dc/bd/2b83372b783db0915ab63e24ac8947169ad70c514ab24ead41\n",
      "Successfully built objax\n",
      "Installing collected packages: parameterized, objax\n",
      "Successfully installed objax-1.3.1 parameterized-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install objax\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import objax\n",
    "from objax.zoo.wide_resnet import WideResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "X_train = X_train.transpose(0, 3, 1, 2) / 255.0\n",
    "X_test = X_test.transpose(0, 3, 1, 2) / 255.0\n",
    "\n",
    "# Model\n",
    "model = WideResNet(nin=3, nclass=10, depth=28, width=2)\n",
    "opt = objax.optimizer.Adam(model.vars())\n",
    "\n",
    "# Losses\n",
    "@objax.Function.with_vars(model.vars())\n",
    "def loss(x, label):\n",
    "    logit = model(x, training=True)\n",
    "    return objax.functional.loss.cross_entropy_logits_sparse(logit, label).mean()\n",
    "\n",
    "gv = objax.GradValues(loss, model.vars())\n",
    "\n",
    "@objax.Function.with_vars(model.vars() + opt.vars())\n",
    "def train_op(x, y, lr):\n",
    "    g, v = gv(x, y)\n",
    "    opt(lr=lr, grads=g)\n",
    "    return v\n",
    "\n",
    "\n",
    "train_op = objax.Jit(train_op)\n",
    "predict = objax.Jit(objax.nn.Sequential([\n",
    "    objax.ForceArgs(model, training=False), objax.functional.softmax\n",
    "]))\n",
    "\n",
    "\n",
    "def augment(x):\n",
    "    if random.random() < .5:\n",
    "        x = x[:, :, :, ::-1]  # Flip the batch images about the horizontal axis\n",
    "    # Pixel-shift all images in the batch by up to 4 pixels in any direction.\n",
    "    x_pad = np.pad(x, [[0, 0], [0, 0], [4, 4], [4, 4]], 'reflect')\n",
    "    rx, ry = np.random.randint(0, 8), np.random.randint(0, 8)\n",
    "    x = x_pad[:, :, rx:rx + 32, ry:ry + 32]\n",
    "    return x\n",
    "\n",
    "\n",
    "# Training\n",
    "# print(model.vars())\n",
    "for epoch in range(30):\n",
    "    # Train\n",
    "    loss = []\n",
    "    sel = np.arange(len(X_train))\n",
    "    np.random.shuffle(sel)\n",
    "    for it in range(0, X_train.shape[0], 64):\n",
    "        loss.append(train_op(augment(X_train[sel[it:it + 64]]), Y_train[sel[it:it + 64]].flatten(),\n",
    "                             4e-3 if epoch < 20 else 4e-4))\n",
    "\n",
    "    # Eval\n",
    "    test_predictions = [predict(x_batch).argmax(1) for x_batch in X_test.reshape((50, -1) + X_test.shape[1:])]\n",
    "    accuracy = np.array(test_predictions).flatten() == Y_test.flatten()\n",
    "    print(f'Epoch {epoch + 1:4d}  Loss {np.mean(loss):.2f}  Accuracy {100 * np.mean(accuracy):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp\n",
    "\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.sum(3 * x ** 2)\n",
    "x = np.ones((2, 3))\n",
    "y, vjp_fun = jax.vjp(f, x)\n",
    "# compute J^T v\n",
    "vjp = vjp_fun(np.array(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[6., 6., 6.],\n",
       "              [6., 6., 6.]], dtype=float32),)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_grad:\n",
      " [[6. 6. 6.]\n",
      " [6. 6. 6.]]\n",
      "jax grad:\n",
      " [[6. 6. 6.]\n",
      " [6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "def my_grad(f, x):\n",
    "  y, vjp_fn = jax.vjp(f, x)\n",
    "  return vjp_fn(np.ones(y.shape))[0]\n",
    "\n",
    "print(\"my_grad:\\n {}\".format(my_grad(f, np.ones((2, 3)))))\n",
    "print(\"jax grad:\\n {}\".format(jax.grad(f)(np.ones((2, 3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3721109   0.26423115 -0.18252768 -0.7368197  -0.44030377 -0.1521442\n",
      " -0.67135346 -0.5908641   0.73168886  0.5673026 ]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp = np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17 ms ± 129 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 µs ± 29.2 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jax.jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
